from typing import List, Dict, Optional
from utils.symptom_mapping import SYMPTOM_MAPPING
from utils.text_cleaner import clean_and_tokenize

TIME_KEYWORDS = {
    "ko": {
        "ì•„ì¹¨": "morning", "ì˜¤ì „": "morning", "ì ì‹¬": "afternoon",
        "ì˜¤í›„": "afternoon", "ì €ë…": "evening", "ë°¤": "night",
    },
    "en": {
        "morning": "morning", "afternoon": "afternoon",
        "evening": "evening", "night": "night",
    },
}

def extract_combined_symptoms(text_ko: str, text_en: str, verbose: bool = False) -> List[Dict[str, str]]:
    if verbose:
        print(f"\nğŸ§¾ [Step 1] ì›ë¬¸(ko): {text_ko}")
        print(f"ğŸ”µ [Step 2] ë²ˆì—­(en): {text_en}")
    
    results = []
    tokens_ko = clean_and_tokenize(text_ko)
    tokens_en = clean_and_tokenize(text_en.lower())

    if verbose:
        print(f"\nğŸŸ¡ [Step 3] Tokenized (ko): {tokens_ko}")
        print(f"ğŸŸ¡ [Step 4] Tokenized (en): {tokens_en}")
        print(f"ğŸŸ¡ [Step 4-1] ì •ì œ í›„ í† í° (ko): {tokens_ko}")
        print(f"ğŸŸ¡ [Step 4-2] ì •ì œ í›„ í† í° (en): {tokens_en}")

    for symptom, mapping in SYMPTOM_MAPPING.items():
        if any(keyword in text_ko for keyword in mapping["ko"]):
            if verbose:
                print(f"âœ… [KO match] '{symptom}' matched by keyword in: {mapping['ko']}")
            results.append({"symptom": symptom, "time": detect_time(text_ko, "ko")})
            continue

        if any(keyword in text_en for keyword in mapping["en"]):
            if verbose:
                print(f"âœ… [EN match] '{symptom}' matched by keyword in: {mapping['en']}")
            results.append({"symptom": symptom, "time": detect_time(text_en, "en")})
            continue

        for token_set in mapping.get("token_sets", []):
            part1_match = [tok for tok in tokens_ko if tok in token_set["part1"]]
            part2_match = [tok for tok in tokens_ko if tok in token_set["part2"]]
            if verbose:
                print(f"ğŸ” [DEBUG] {symptom} token_sets ê²€ì‚¬ - part1: {part1_match}, part2: {part2_match}")
            if part1_match and part2_match:
                if verbose:
                    print(f"âœ… [TokenSet match] '{symptom}' matched by part1: {part1_match}, part2: {part2_match}")
                results.append({"symptom": symptom, "time": detect_time(text_ko, "ko")})
                break

    composite = handle_composite_symptoms(text_ko, results)
    if composite:
        if verbose:
            print(f"âœ… [Composite] Added symptoms from 'ëª¸ì‚´': {[s['symptom'] for s in composite]}")
        results += composite

    final = deduplicate_results(results)
    if verbose:
        print(f"\nğŸŸ¢ [Step 5] ìµœì¢… ì¶”ì¶œ ê²°ê³¼: {final}")
    return final


def detect_time(text: str, lang: str) -> Optional[str]:
    return next((v for k, v in TIME_KEYWORDS[lang].items() if k in text), None)

def deduplicate_results(results: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen, unique = set(), []
    for item in results:
        key = (item["symptom"], item["time"])
        if key not in seen:
            seen.add(key)
            unique.append(item)
    return unique

def handle_composite_symptoms(text_ko: str, existing_results: List[Dict[str, str]]) -> List[Dict[str, str]]:
    composite_symptoms = []
    if "ëª¸ì‚´" in text_ko:
        # ëª¸ì‚´ë¡œ ìœ ë°œë˜ëŠ” ì¦ìƒ í‘œí˜„
        related_ko_keywords = ["ì˜¤í•œ", "ê·¼ìœ¡í†µ", "í”¼ë¡œ", "ì—´"]

        for keyword in related_ko_keywords:
            matched_id = None
            for sid, mapping in SYMPTOM_MAPPING.items():
                if keyword in mapping["ko"]:
                    matched_id = sid
                    break

            if matched_id and not any(r["symptom"] == matched_id for r in existing_results):
                composite_symptoms.append({
                    "symptom": matched_id,
                    "time": detect_time(text_ko, "ko")
                })

    return composite_symptoms
